# ELEET: Evaluating LLMs on Coding Questions

**ELEET** is an automated system for testing how well large language models (LLMs) solve LeetCode-style coding problems. It handles prompt creation, model querying, code extraction, test execution, and result loggingâ€”all with minimal manual effort.

![Workflow](./WorkflowCroppedELEETC.png)

## ğŸ”§ Files

- `Final_TestScript.py` â€“ Main script for running evaluations
- `test_problems_video.csv` â€“ Backend database of questions + test cases
- `Final_Report.csv` â€“ Output report for model results (initially empty)

## âš™ï¸ How It Works

1. **Add Questions** to the CSV using the format: description, constraints, function signature, test cases  
2. **Configure Model/API** â€“ Select model type and enter key (if needed)  
3. **Choose Question** â€“ Pick from the list or let the system choose randomly  
4. **Define Prompt** â€“ Use default or enter a custom prompt  
5. **Set # of Responses** â€“ Specify how many outputs to generate  
6. **Query Model** â€“ Responses are fetched via API  
7. **Run Tests** â€“ Code is parsed and tested on predefined cases  
8. **Export Results** â€“ Save outputs to `Final_Report.csv`

## âœ… Output Includes

- Prompt used  
- Model name  
- Code generated  
- Whether all test cases passed  

## ğŸ Run It

```bash
python Final_TestScript.py
